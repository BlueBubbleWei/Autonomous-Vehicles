{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "from moviepy.editor import VideoFileClip, ImageSequenceClip\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(helper)\n",
    "\n",
    "cal_path = 'camera_cal/'\n",
    "output_images = 'output_images/'\n",
    "path = 'test_images/'\n",
    "\n",
    "def pipeline(img, mtx, dist, roi_vertices, M, M_inv, left_line, right_line, bad_frames):\n",
    "    # Undistort image\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    imshape = undist.shape\n",
    "\n",
    "    # Apply thresholds to image and combine them\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 5\n",
    "    \n",
    "    # Convert to HLS color space and separate the S channel\n",
    "    hls = cv2.cvtColor(undist, cv2.COLOR_RGB2HLS)\n",
    "    h_channel = hls[...,0]\n",
    "    h_channel = helper.clahe(h_channel, 4, (5, 5))\n",
    "    l_channel = hls[...,1]\n",
    "    l_channel = helper.clahe(l_channel, 6, (15, 15))\n",
    "    s_channel = hls[...,2]\n",
    "    s_channel = helper.clahe(s_channel, 4, (5, 5))\n",
    "\n",
    "    # Apply thresholds to image and combine them\n",
    "    # Choose a Sobel kernel size\n",
    "    ksize = 7\n",
    "    close_kernel = np.ones((5,5),np.uint8)\n",
    "\n",
    "    gradx = helper.abs_sobel_thresh(s_channel, orient='x', kernel=ksize,\n",
    "                                    thresh=(50, 200))\n",
    "    gradx = cv2.morphologyEx(gradx, cv2.MORPH_CLOSE, kernel=close_kernel)\n",
    "    grady = helper.abs_sobel_thresh(s_channel, orient='y', kernel=ksize,\n",
    "                                    thresh=(70, 200))\n",
    "    mag_binary = helper.mag_thresh(s_channel, kernel=ksize, thresh=(30, 200))\n",
    "    gradients = np.zeros_like(gradx)\n",
    "    gradients[((gradx == 1) | (grady == 1)) & (mag_binary == 1)] = 1\n",
    "    \n",
    "    yellow_min = np.array([15, 60, 60], np.uint8)\n",
    "    yellow_max = np.array([90, 200, 255], np.uint8)\n",
    "    yellow_mask = cv2.inRange(hls, yellow_min, yellow_max)\n",
    "    \n",
    "    white_min = np.array([0, 200, 0], np.uint8)\n",
    "    white_max = np.array([255, 255, 255], np.uint8)\n",
    "    white_mask = cv2.inRange(hls, white_min, white_max)\n",
    "\n",
    "    binary_output = np.zeros_like(gradients)\n",
    "    binary_output[((yellow_mask == 255) | (white_mask == 255))] = 1\n",
    "\n",
    "    color_binary_h = helper.color_threshold(h_channel, thresh=(30, 80))\n",
    "    color_binary_l = helper.color_threshold(l_channel, thresh=(170, 255))\n",
    "    color_binary_s = helper.color_threshold(s_channel, thresh=(100, 255))\n",
    "\n",
    "    color_combined = np.zeros_like(binary_output)\n",
    "    color_combined[(color_binary_h == 1) | ((color_binary_s == 1) | (color_binary_l == 1))] = 1\n",
    "\n",
    "    # Combine gradient and color thresholding\n",
    "    mask = np.zeros_like(binary_output)\n",
    "    mask[(binary_output == 1) & ((gradients == 1) | (color_combined == 1))] = 1\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel=close_kernel)\n",
    "\n",
    "    roi = helper.region_of_interest(mask, [roi_vertices])\n",
    "    warped = cv2.warpPerspective(roi, M, roi.shape[::-1])\n",
    "\n",
    "    # Convolution over warped image to find hot pixels defining lane lines\n",
    "    conv_kernel = np.ones((5,5),np.uint8)\n",
    "    conv = helper.conv_img(warped, conv_kernel)\n",
    "    \n",
    "    if (not left_line.detected or not right_line.detected) and bad_frames > 2 or \\\n",
    "    left_line.best_fit is None or right_line.best_fit is None:\n",
    "        # Calculate initial lane centroids\n",
    "        initial_fit = True\n",
    "        left_x_centroids, left_y_centroids, \\\n",
    "        right_x_centroids, right_y_centroids = helper.window_search(conv) \n",
    "    else:\n",
    "        # Calculate lane centroids using previous ones as support\n",
    "        left_x_centroids, left_y_centroids = helper.guided_window_search(conv, left_line)\n",
    "        right_x_centroids, right_y_centroids = helper.guided_window_search(conv, right_line)\n",
    "\n",
    "    \n",
    "    # Calculate base position and update lines\n",
    "    ym_per_pix = 30 / 720\n",
    "    xm_per_pix = 3.7 / 790\n",
    "\n",
    "    halfway = 3.7 / 2\n",
    "    car_pos = conv.shape[1] / 2\n",
    "    if left_line.best_fit is None or right_line.best_fit is None:\n",
    "        left_base_pos = None\n",
    "        right_base_pos = None\n",
    "        line_base_pos = 0\n",
    "    else:\n",
    "        left_pos = np.polyval(left_line.best_fit, conv.shape[1])\n",
    "        right_pos = np.polyval(right_line.best_fit, conv.shape[1])\n",
    "        lane_px = abs(left_pos - right_pos)\n",
    "        lane_midpoint = lane_px / 2 + left_pos\n",
    "        line_base_pos = (car_pos - lane_midpoint) * xm_per_pix\n",
    "        left_base_pos = halfway + line_base_pos\n",
    "        right_base_pos = halfway - line_base_pos\n",
    "\n",
    "    # Update lines\n",
    "    left_line = helper.update_line(conv, left_line, left_x_centroids, \\\n",
    "                                   left_y_centroids, left_base_pos)\n",
    "\n",
    "    right_line = helper.update_line(conv, right_line, right_x_centroids, \\\n",
    "                                    right_y_centroids, right_base_pos)\n",
    "    \n",
    "    # Unwarp image\n",
    "    ploty = np.linspace(0, conv.shape[0]-1, conv.shape[0])\n",
    "    left_fitx = np.polyval(left_line.best_fit, ploty)\n",
    "    right_fitx = np.polyval(right_line.best_fit, ploty)\n",
    "    left_zipped = np.array(list(zip(left_fitx, ploty)), dtype=np.int32)\n",
    "    right_zipped = np.array(list(zip(right_fitx, ploty)), dtype=np.int32)\n",
    "    \n",
    "    fitted = np.zeros_like(conv)\n",
    "    fitted = np.dstack((fitted, fitted, fitted))\n",
    "    cv2.fillPoly(fitted, [np.concatenate((left_zipped, right_zipped[::-1]))],\n",
    "             (0,255, 0))\n",
    "    \n",
    "    # Uncomment to get a video side by side with the top-view of the lane\n",
    "    # with the algorithm working\n",
    "    \n",
    "#     aux_fitted = np.zeros_like(fitted)\n",
    "    \n",
    "#     cv2.polylines(aux_fitted, [left_zipped], False, (0,0,255), 6, -1)\n",
    "#     cv2.polylines(aux_fitted, [right_zipped], False, (0,0,255), 6, -1)\n",
    "    \n",
    "#     for values in left_line.recent_centroids_fitted:\n",
    "#         for centroid in values:\n",
    "#             cv2.circle(aux_fitted, (int(centroid[0]), int(centroid[1])),\n",
    "#                        6, (0,255,0), -1)\n",
    "#     for values in right_line.recent_centroids_fitted:\n",
    "#         for centroid in values:\n",
    "#             cv2.circle(aux_fitted, (int(centroid[0]), int(centroid[1])),\n",
    "#                        6, (0,255,0), -1)\n",
    "    \n",
    "    unwarped = cv2.warpPerspective(fitted, M_inv, fitted.shape[1::-1])\n",
    "    \n",
    "    curverad = (left_line.radius_of_curvature + right_line.radius_of_curvature) / 2\n",
    "    curverad = round(curverad, 2)\n",
    "    cv2.putText(unwarped, 'Radius of curvature: ' + str(curverad) + 'm',\n",
    "                (40, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255))\n",
    "\n",
    "    line_base_pos = round(line_base_pos, 2)\n",
    "    if line_base_pos < 0:\n",
    "        cv2.putText(unwarped, 'Base position: ' + str(-line_base_pos) + \n",
    "                    'm left of center', (40, 60), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    (255,255,255))\n",
    "    else:\n",
    "        cv2.putText(unwarped, 'Base position: ' + str(line_base_pos) + \n",
    "                    'm right of center', (40, 60), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    (255,255,255))\n",
    "    \n",
    "    weighted = cv2.addWeighted(unwarped, 0.4, undist, 1., 0.)\n",
    "    \n",
    "    # Uncomment to get a video side by side with the top-view of the lane\n",
    "    # with the algorithm working\n",
    "    \n",
    "#     aux_conv = np.dstack((conv, conv, conv))*128\n",
    "#     aux_weighted = cv2.addWeighted(aux_fitted, 0.4, aux_conv, 1., 0.)\n",
    "#     weighted = np.hstack((weighted, aux_weighted))\n",
    "    \n",
    "    return weighted, left_line, right_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'camera_calibration'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-408ba08aec55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcamera_calibration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbad_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test_videos_output./output.mp4'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# input_video = './project_video.mp4'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'camera_calibration'"
     ]
    }
   ],
   "source": [
    "mtx, dist = helper.camera_calibration(cal_path, output_images)\n",
    "\n",
    "bad_frames = 0\n",
    "output_path = 'test_videos_output./output.mp4'\n",
    "# input_video = './project_video.mp4'\n",
    "input_video = 'test_videos/challenge.mp4'\n",
    "clip = VideoFileClip(input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'Line'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-872451a979a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhelper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mleft_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mright_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mout_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'Line'"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import helper\n",
    "reload(helper)\n",
    "\n",
    "left_line = helper.Line()\n",
    "right_line = helper.Line()\n",
    "out_frames = []\n",
    "loops = 0\n",
    "\n",
    "temp_dir = './temp/'\n",
    "if not os.path.isdir(temp_dir):\n",
    "    os.mkdir(temp_dir)\n",
    "\n",
    "for frame in clip.iter_frames():\n",
    "\n",
    "    new_frame, left_line, right_line = pipeline(frame, mtx, dist, roi_vertices,\n",
    "                                                M, M_inv, left_line, right_line,\n",
    "                                                bad_frames)\n",
    "    if not left_line.detected or not right_line.detected:\n",
    "        bad_frames += 1\n",
    "    else:\n",
    "        bad_frames = 0\n",
    "\n",
    "    frame_name = temp_dir + 'temp_'+str(loops)+'.jpg'\n",
    "    cv2.imwrite(frame_name, cv2.cvtColor(new_frame, cv2.COLOR_RGB2BGR))\n",
    "    loops += 1\n",
    "    out_frames.append(frame_name)\n",
    "\n",
    "out_clip = ImageSequenceClip(out_frames, fps=clip.fps)\n",
    "%time out_clip.write_videofile(output_path, audio=False)\n",
    "# Comment the line below to keep the images generated by the pipeline\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
